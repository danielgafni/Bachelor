@misc{saunders2019locally,
	archiveprefix = {arXiv},
	author = {Saunders, Daniel J. and Patel, Devdhar and Hazan, Hananel and Siegelmann, Hava T. and Kozma, Robert},
	eprint = {1904.06269},
	primaryclass = {cs.NE},
	title = {Locally Connected Spiking Neural Networks for Unsupervised Feature Learning},
	year = {2019}
}

@inproceedings{pmlr-v28-wan13,
	abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.},
	address = {Atlanta, Georgia, USA},
	author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann Le and Fergus, Rob},
	booktitle = {Proceedings of the 30th International Conference on Machine Learning},
	editor = {Dasgupta, Sanjoy and McAllester, David},
	month = {17--19 Jun},
	number = {3},
	pages = {1058--1066},
	pdf = {http://proceedings.mlr.press/v28/wan13.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Regularization of Neural Networks using DropConnect},
	url = {http://proceedings.mlr.press/v28/wan13.html},
	volume = {28},
	year = {2013}
}

@article{MaxActiv1,
	author = {Demin, Vyacheslav and Nekhaev, Dmitry},
	doi = {10.3389/fninf.2018.00079},
	journal = {Frontiers in Neuroinformatics},
	month = {11},
	pages = {79},
	title = {Recurrent Spiking Neural Network Learning Based on a Competitive Maximization of Neuronal Activity},
	volume = {12},
	year = {2018}
}

@inbook{MaxActiv2,
	author = {Nekhaev, Dmitry and Demin, Vyacheslav},
	doi = {10.1007/978-3-030-30425-6_30},
	isbn = {978-3-030-30424-9},
	month = {01},
	pages = {255--262},
	title = {Competitive Maximization of Neuronal Activity in Convolutional Recurrent Spiking Neural Networks},
	year = {2020}
}

@article{ManyParams,
author = {Alom, Md. Zahangir and Taha, Tarek and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst and Hasan, Mahmudul and Essen, Brian and Awwal, Abdul and Asari, Vijayan},
year = {2019},
month = {03},
pages = {292},
title = {A State-of-the-Art Survey on Deep Learning Theory and Architectures},
volume = {8},
journal = {Electronics},
doi = {10.3390/electronics8030292}
}

@ARTICLE{STDP,
  
AUTHOR={Markram, Henry and Gerstner, Wulfram and Sjöström, Per Jesper},   
	 
TITLE={A History of Spike-Timing-Dependent Plasticity},      
	
JOURNAL={Frontiers in Synaptic Neuroscience},      
	
VOLUME={3},      

PAGES={4},     
	
YEAR={2011},      
	  
URL={https://www.frontiersin.org/article/10.3389/fnsyn.2011.00004},       
	
DOI={10.3389/fnsyn.2011.00004},      
	
ISSN={1663-3563},   
   
ABSTRACT={How learning and memory is achieved in the brain is a central question in neuroscience. Key to today’s research into information storage in the brain is the concept of synaptic plasticity, a notion that has been heavily influenced by Hebb's (<xref ref-type="bibr" rid="B97">1949</xref>) postulate. Hebb conjectured that repeatedly and persistently co-active cells should increase connective strength among populations of interconnected neurons as a means of storing a memory trace, also known as an engram. Hebb certainly was not the first to make such a conjecture, as we show in this history. Nevertheless, literally thousands of studies into the classical frequency-dependent paradigm of cellular learning rules were directly inspired by the Hebbian postulate. But in more recent years, a novel concept in cellular learning has emerged, where temporal order instead of frequency is emphasized. This new learning paradigm – known as spike-timing-dependent plasticity (STDP) – has rapidly gained tremendous interest, perhaps because of its combination of elegant simplicity, biological plausibility, and computational power. But what are the roots of today’s STDP concept? Here, we discuss several centuries of diverse thinking, beginning with philosophers such as Aristotle, Locke, and Ribot, traversing, e.g., Lugaro’s plasticità and Rosenblatt’s perceptron, and culminating with the discovery of STDP. We highlight interactions between theoretical and experimental fields, showing how discoveries sometimes occurred in parallel, seemingly without much knowledge of the other field, and sometimes via concrete back-and-forth communication. We point out where the future directions may lie, which includes interneuron STDP, the functional impact of STDP, its mechanisms and its neuromodulatory regulation, and the linking of STDP to the developmental formation and continuous plasticity of neuronal networks.}
}

@article{LOBO202088,
title = {Spiking Neural Networks and online learning: An overview and perspectives},
journal = {Neural Networks},
volume = {121},
pages = {88 - 100},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.004},
url = {http://www.sciencedirect.com/science/article/pii/S0893608019302655},
author = {Jesus L. Lobo and Javier [Del Ser] and Albert Bifet and Nikola Kasabov},
keywords = {Online learning, Spiking Neural Networks, Stream data, Concept drift},
abstract = {Applications that generate huge amounts of data in the form of fast streams are becoming increasingly prevalent, being therefore necessary to learn in an online manner. These conditions usually impose memory and processing time restrictions, and they often turn into evolving environments where a change may affect the input data distribution. Such a change causes that predictive models trained over these stream data become obsolete and do not adapt suitably to new distributions. Specially in these non-stationary scenarios, there is a pressing need for new algorithms that adapt to these changes as fast as possible, while maintaining good performance scores. Unfortunately, most off-the-shelf classification models need to be retrained if they are used in changing environments, and fail to scale properly. Spiking Neural Networks have revealed themselves as one of the most successful approaches to model the behavior and learning potential of the brain, and exploit them to undertake practical online learning tasks. Besides, some specific flavors of Spiking Neural Networks can overcome the necessity of retraining after a drift occurs. This work intends to merge both fields by serving as a comprehensive overview, motivating further developments that embrace Spiking Neural Networks for online learning scenarios, and being a friendly entry point for non-experts.}
}
