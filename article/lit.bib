@misc{saunders2019locally,
	archiveprefix = {arXiv},
	author = {Saunders, Daniel J. and Patel, Devdhar and Hazan, Hananel and Siegelmann, Hava T. and Kozma, Robert},
	eprint = {1904.06269},
	primaryclass = {cs.NE},
	title = {Locally Connected Spiking Neural Networks for Unsupervised Feature Learning},
	year = {2019}
}

@inproceedings{pmlr-v28-wan13,
	abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.},
	address = {Atlanta, Georgia, USA},
	author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann Le and Fergus, Rob},
	booktitle = {Proceedings of the 30th International Conference on Machine Learning},
	editor = {Dasgupta, Sanjoy and McAllester, David},
	month = {17--19 Jun},
	number = {3},
	pages = {1058--1066},
	pdf = {http://proceedings.mlr.press/v28/wan13.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Regularization of Neural Networks using DropConnect},
	url = {http://proceedings.mlr.press/v28/wan13.html},
	volume = {28},
	year = {2013}
}

@article{MaxActiv1,
	author = {Demin, Vyacheslav and Nekhaev, Dmitry},
	doi = {10.3389/fninf.2018.00079},
	journal = {Frontiers in Neuroinformatics},
	month = {11},
	pages = {79},
	title = {Recurrent Spiking Neural Network Learning Based on a Competitive Maximization of Neuronal Activity},
	volume = {12},
	year = {2018}
}

@inbook{MaxActiv2,
	author = {Nekhaev, Dmitry and Demin, Vyacheslav},
	doi = {10.1007/978-3-030-30425-6_30},
	isbn = {978-3-030-30424-9},
	month = {01},
	pages = {255--262},
	title = {Competitive Maximization of Neuronal Activity in Convolutional Recurrent Spiking Neural Networks},
	year = {2020}
}

@article{ManyParams,
author = {Alom, Md. Zahangir and Taha, Tarek and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst and Hasan, Mahmudul and Essen, Brian and Awwal, Abdul and Asari, Vijayan},
year = {2019},
month = {03},
pages = {292},
title = {A State-of-the-Art Survey on Deep Learning Theory and Architectures},
volume = {8},
journal = {Electronics},
doi = {10.3390/electronics8030292}
}

@ARTICLE{STDP,
  
AUTHOR={Markram, Henry and Gerstner, Wulfram and Sjöström, Per Jesper},   
	 
TITLE={A History of Spike-Timing-Dependent Plasticity},      
	
JOURNAL={Frontiers in Synaptic Neuroscience},      
	
VOLUME={3},      

PAGES={4},     
	
YEAR={2011},      
	  
URL={https://www.frontiersin.org/article/10.3389/fnsyn.2011.00004},       
	
DOI={10.3389/fnsyn.2011.00004},      
	
ISSN={1663-3563},   
   
ABSTRACT={How learning and memory is achieved in the brain is a central question in neuroscience. Key to today’s research into information storage in the brain is the concept of synaptic plasticity, a notion that has been heavily influenced by Hebb's (<xref ref-type="bibr" rid="B97">1949</xref>) postulate. Hebb conjectured that repeatedly and persistently co-active cells should increase connective strength among populations of interconnected neurons as a means of storing a memory trace, also known as an engram. Hebb certainly was not the first to make such a conjecture, as we show in this history. Nevertheless, literally thousands of studies into the classical frequency-dependent paradigm of cellular learning rules were directly inspired by the Hebbian postulate. But in more recent years, a novel concept in cellular learning has emerged, where temporal order instead of frequency is emphasized. This new learning paradigm – known as spike-timing-dependent plasticity (STDP) – has rapidly gained tremendous interest, perhaps because of its combination of elegant simplicity, biological plausibility, and computational power. But what are the roots of today’s STDP concept? Here, we discuss several centuries of diverse thinking, beginning with philosophers such as Aristotle, Locke, and Ribot, traversing, e.g., Lugaro’s plasticità and Rosenblatt’s perceptron, and culminating with the discovery of STDP. We highlight interactions between theoretical and experimental fields, showing how discoveries sometimes occurred in parallel, seemingly without much knowledge of the other field, and sometimes via concrete back-and-forth communication. We point out where the future directions may lie, which includes interneuron STDP, the functional impact of STDP, its mechanisms and its neuromodulatory regulation, and the linking of STDP to the developmental formation and continuous plasticity of neuronal networks.}
}

