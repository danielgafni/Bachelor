@article{neuronmodels,
	title        = {Spiking Neuron Models: A Review},
	author       = {Abusnaina, Ahmed and Abdullah, Rosni},
	year         = 2014,
	month        = {06},
	journal      = {International Journal of Digital Content Technology and its Applications},
	volume       = 8,
	pages        = {14--21}
}
@article{ManyParams,
	title        = {A State-of-the-Art Survey on Deep Learning Theory and Architectures},
	author       = {Alom, Md. Zahangir and Taha, Tarek and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst and Hasan, Mahmudul and Essen, Brian and Awwal, Abdul and Asari, Vijayan},
	year         = 2019,
	month        = {03},
	journal      = {Electronics},
	volume       = 8,
	pages        = 292,
	doi          = {10.3390/electronics8030292}
}
@article{hardware_survey,
	title        = {Spiking Neural Networks Hardware Implementations and Challenges},
	author       = {Bouvier, Maxence and Valentian, Alexandre and Mesquida, Thomas and Rummens, Francois and Reyboz, Marina and Vianello, Elisa and Beigne, Edith},
	year         = 2019,
	month        = jun,
	journal      = {ACM Journal on Emerging Technologies in Computing Systems},
	publisher    = {Association for Computing Machinery (ACM)},
	volume       = 15,
	number       = 2,
	pages        = {1--35},
	doi          = {10.1145/3304103},
	issn         = {1550-4840},
	url          = {http://dx.doi.org/10.1145/3304103}
}
@article{Loihi,
	title        = {Loihi: A Neuromorphic Manycore Processor with On-Chip Learning},
	author       = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Joshi, Prasad and Lines, Andrew and Wild, Andreas and Wang, Hong},
	year         = 2018,
	month        = {01},
	journal      = {IEEE Micro},
	volume       = {PP},
	pages        = {1--1},
	doi          = {10.1109/MM.2018.112130359}
}
@article{saunders2019locally,
	title        = {Locally Connected Spiking Neural Networks for Unsupervised Feature Learning},
	author       = {Saunders, Daniel J. and Patel, Devdhar and Hazan, Hananel and Siegelmann, Hava T. and Kozma, Robert},
	year         = 2019,
	archiveprefix = {arXiv},
	eprint       = {1904.06269},
	primaryclass = {cs.NE}
}
@article{pmlr-v28-wan13,
	title        = {Regularization of Neural Networks using DropConnect},
	author       = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann Le and Fergus, Rob},
	year         = 2013,
	month        = {06},
	booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Atlanta, Georgia, USA},
	series       = {Proceedings of Machine Learning Research},
	volume       = 28,
	number       = 3,
	pages        = {1058--1066},
	url          = {http://proceedings.mlr.press/v28/wan13.html},
	abstract     = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.},
	editor       = {Dasgupta, Sanjoy and McAllester, David},
	pdf          = {http://proceedings.mlr.press/v28/wan13.pdf}
}
@article{MaxActiv1,
	title        = {Recurrent Spiking Neural Network Learning Based on a Competitive Maximization of Neuronal Activity},
	author       = {Demin, Vyacheslav and Nekhaev, Dmitry},
	year         = 2018,
	month        = 11,
	journal      = {Frontiers in Neuroinformatics},
	volume       = 12,
	pages        = 79,
	doi          = {10.3389/fninf.2018.00079}
}
@article{MaxActiv2,
	title        = {Competitive Maximization of Neuronal Activity in Convolutional Recurrent Spiking Neural Networks},
	author       = {Nekhaev, Dmitry and Demin, Vyacheslav},
	year         = 2020,
	month        = {01},
	pages        = {255--262},
	doi          = {10.1007/978-3-030-30425-6_30},
	isbn         = {978-3-030-30424-9}
}
@article{STDP,
	title        = {A History of Spike-Timing-Dependent Plasticity},
	author       = {Markram, Henry and Gerstner, Wulfram and Sj{\"o}str{\"o}m, Per Jesper},
	year         = 2011,
	journal      = {Frontiers in Synaptic Neuroscience},
	volume       = 3,
	pages        = 4,
	doi          = {10.3389/fnsyn.2011.00004},
	issn         = {1663-3563},
	url          = {https://www.frontiersin.org/article/10.3389/fnsyn.2011.00004},
	abstract     = {How learning and memory is achieved in the brain is a central question in neuroscience. Key to today{\rq}s research into information storage in the brain is the concept of synaptic plasticity, a notion that has been heavily influenced by Hebb's (<xref ref-type="bibr" rid="B97">1949</xref>) postulate. Hebb conjectured that repeatedly and persistently co-active cells should increase connective strength among populations of interconnected neurons as a means of storing a memory trace, also known as an engram. Hebb certainly was not the first to make such a conjecture, as we show in this history. Nevertheless, literally thousands of studies into the classical frequency-dependent paradigm of cellular learning rules were directly inspired by the Hebbian postulate. But in more recent years, a novel concept in cellular learning has emerged, where temporal order instead of frequency is emphasized. This new learning paradigm -- known as spike-timing-dependent plasticity (STDP) -- has rapidly gained tremendous interest, perhaps because of its combination of elegant simplicity, biological plausibility, and computational power. But what are the roots of today{\rq}s STDP concept? Here, we discuss several centuries of diverse thinking, beginning with philosophers such as Aristotle, Locke, and Ribot, traversing, e.g., Lugaro{\rq}s plasticit{\`a} and Rosenblatt{\rq}s perceptron, and culminating with the discovery of STDP. We highlight interactions between theoretical and experimental fields, showing how discoveries sometimes occurred in parallel, seemingly without much knowledge of the other field, and sometimes via concrete back-and-forth communication. We point out where the future directions may lie, which includes interneuron STDP, the functional impact of STDP, its mechanisms and its neuromodulatory regulation, and the linking of STDP to the developmental formation and continuous plasticity of neuronal networks.}
}
@article{LOBO202088,
	title        = {Spiking Neural Networks and online learning: An overview and perspectives},
	author       = {Lobo, Jesus L. and Ser], Javier [Del and Bifet, Albert and Kasabov, Nikola},
	year         = 2020,
	journal      = {Neural Networks},
	volume       = 121,
	pages        = {88--100},
	doi          = {10.1016/j.neunet.2019.09.004},
	issn         = {0893-6080},
	url          = {http://www.sciencedirect.com/science/article/pii/S0893608019302655},
	abstract     = {Applications that generate huge amounts of data in the form of fast streams are becoming increasingly prevalent, being therefore necessary to learn in an online manner. These conditions usually impose memory and processing time restrictions, and they often turn into evolving environments where a change may affect the input data distribution. Such a change causes that predictive models trained over these stream data become obsolete and do not adapt suitably to new distributions. Specially in these non-stationary scenarios, there is a pressing need for new algorithms that adapt to these changes as fast as possible, while maintaining good performance scores. Unfortunately, most off-the-shelf classification models need to be retrained if they are used in changing environments, and fail to scale properly. Spiking Neural Networks have revealed themselves as one of the most successful approaches to model the behavior and learning potential of the brain, and exploit them to undertake practical online learning tasks. Besides, some specific flavors of Spiking Neural Networks can overcome the necessity of retraining after a drift occurs. This work intends to merge both fields by serving as a comprehensive overview, motivating further developments that embrace Spiking Neural Networks for online learning scenarios, and being a friendly entry point for non-experts.},
	keywords     = {Online learning, Spiking Neural Networks, Stream data, Concept drift}
}
@article{hardware1,
	title        = {Darwin: A neuromorphic hardware co-processor based on spiking neural networks},
	author       = {Ma, De and Shen, Juncheng and Gu, Zonghua and Zhang, Ming and Zhu, Xiaolei and Xu, Xiaoqiang and Xu, Qi and Shen, Yangjing and Pan, Gang},
	year         = 2017,
	journal      = {Journal of Systems Architecture},
	volume       = 77,
	pages        = {43--51},
	doi          = {10.1016/j.sysarc.2017.01.003},
	issn         = {1383-7621},
	url          = {http://www.sciencedirect.com/science/article/pii/S1383762117300231},
	abstract     = {Spiking Neural Network (SNN) is a type of biologically-inspired neural networks that perform information processing based on discrete-time spikes, different from traditional Artificial Neural Network (ANN). Hardware implementation of SNNs is necessary for achieving high-performance and low-power. We present the Darwin Neural Processing Unit (NPU), a highly-configurable neuromorphic hardware co-processor based on SNN implemented with digital logic, supporting a configurable number of neurons, synapses and synaptic delays. The Darwin NPU was fabricated by standard 180nm CMOS technology with area size of 5\,{\texttimes}\,5 mm2 and 70MHz clock frequency at the worst case. It consumes 0.84mW/MHz with 1.8V power supply for typical applications. Two prototype applications are used to demonstrate the performance and efficiency of the Darwin NPU.},
	keywords     = {Neuromorphic computing, Spiking neural networks (SNN), Address-event representation (AER), Digital VLSI}
}
@article{hardware2,
	title        = {Spiking Neural Network on Neuromorphic Hardware for Energy-Efficient Unidimensional SLAM},
	author       = {Tang, Guangzhi and Shah, Arpit and Michmizos, Konstantinos P.},
	year         = 2019,
	archiveprefix = {arXiv},
	eprint       = {1903.02504},
	primaryclass = {cs.RO}
}
@article{mnist1,
	title        = {Spiking neural networks for handwritten digit recognition---Supervised learning and network optimization},
	author       = {Kulkarni, Shruti and Rajendran, Bipin},
	year         = 2018,
	month        = {04},
	journal      = {Neural Networks},
	volume       = 103,
	pages        = {},
	doi          = {10.1016/j.neunet.2018.03.019}
}
@article{TrueNorth,
	title        = {A million spiking-neuron integrated circuit with a scalable communication network and interface},
	author       = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
	year         = 2014,
	journal      = {Science},
	publisher    = {American Association for the Advancement of Science},
	volume       = 345,
	number       = 6197,
	pages        = {668--673},
	doi          = {10.1126/science.1254642},
	issn         = {0036-8075},
	url          = {https://science.sciencemag.org/content/345/6197/668},
	abstract     = {Computers are nowhere near as versatile as our own brains. Merolla et al. applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.Science, this issue p. 668 Inspired by the brain{\rq}s structure, we have developed an efficient, scalable, and flexible non--von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
	eprint       = {https://science.sciencemag.org/content/345/6197/668.full.pdf}
}
@article{SpiNNaker,
	title        = {SpiNNaker: A 1-W 18-Core System-on-Chip for Massively-Parallel Neural Network Simulation},
	author       = {Painkras, Eustace and Plana, Luis and Garside, Jim and Temple, Steve and Galluppi, Francesco and Patterson, Cameron and Lester, David and Brown, Andrew and Furber, Steve},
	year         = 2013,
	month        = {08},
	journal      = {Solid-State Circuits, IEEE Journal of},
	volume       = 48,
	pages        = {1943--1953},
	doi          = {10.1109/JSSC.2013.2259038}
}
@article{Akida,
	title        = {Akida Neural Processor System-on-Chip},
	author       = {},
	year         = 2020,
	month        = apr,
	url          = {https://brainchipinc.com/akida-neuromorphic-system-on-chip/}
}
@article{mnist2,
	title        = {Unsupervised learning of digit recognition using spike-timing-dependent plasticity},
	author       = {Diehl, Peter and Cook, Matthew},
	year         = 2015,
	journal      = {Frontiers in Computational Neuroscience},
	volume       = 9,
	pages        = 99,
	doi          = {10.3389/fncom.2015.00099},
	issn         = {1662-5188},
	url          = {https://www.frontiersin.org/article/10.3389/fncom.2015.00099},
	abstract     = {In order to understand how the mammalian neocortex is performing computations, two things are necessary; we need to have a good understanding of the available neuronal processing units and mechanisms, and we need to gain a better understanding of how those mechanisms are combined to build functioning systems. Therefore, in recent years there is an increasing interest in how spiking neural networks (SNN) can be used to perform complex computations or solve pattern recognition tasks. However, it remains a challenging task to design SNNs which use biologically plausible mechanisms (especially for learning new patterns), since most such SNN architectures rely on training in a rate-based network and subsequent conversion to a SNN. We present a SNN for digit recognition which is based on mechanisms with increased biological plausibility, i.e., conductance-based instead of current-based synapses, spike-timing-dependent plasticity with time-dependent weight change, lateral inhibition, and an adaptive spiking threshold. Unlike most other systems, we do not use a teaching signal and do not present any class labels to the network. Using this unsupervised learning scheme, our architecture achieves 95\% accuracy on the MNIST benchmark, which is better than previous SNN implementations without supervision. The fact that we used no domain-specific knowledge points toward the general applicability of our network design. Also, the performance of our network scales well with the number of neurons used and shows similar performance for four different learning rules, indicating robustness of the full combination of mechanisms, which suggests applicability in heterogeneous biological neural networks.}
}
@article{conv1,
	title        = {Multi-layer unsupervised learning in a spiking convolutional neural network},
	author       = {{Tavanaei}, A. and {Maida}, A. S.},
	year         = 2017,
	booktitle    = {2017 International Joint Conference on Neural Networks (IJCNN)},
	volume       = {},
	number       = {},
	pages        = {2023--2030}
}
@article{conv2,
	title        = {Training Deep Spiking Convolutional Neural Networks With STDP-Based Unsupervised Pre-training Followed by Supervised Fine-Tuning},
	author       = {Lee, Chankyu and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
	year         = 2018,
	journal      = {Frontiers in Neuroscience},
	volume       = 12,
	pages        = 435,
	doi          = {10.3389/fnins.2018.00435},
	issn         = {1662-453X},
	url          = {https://www.frontiersin.org/article/10.3389/fnins.2018.00435},
	abstract     = {Spiking Neural Networks (SNNs) are fast becoming a promising candidate for brain-inspired neuromorphic computing because of their inherent power efficiency and impressive inference accuracy across several cognitive tasks such as image classification and speech recognition. The recent efforts in SNNs have been focused on implementing deeper networks with multiple hidden layers to incorporate exponentially more difficult functional representations. In this paper, we propose a pre-training scheme using biologically plausible unsupervised learning, namely Spike-Timing-Dependent-Plasticity (STDP), in order to better initialize the parameters in multi-layer systems prior to supervised optimization. The multi-layer SNN is comprised of alternating convolutional and pooling layers followed by fully-connected layers, which are populated with leaky integrate-and-fire spiking neurons. We train the deep SNNs in two phases wherein, first, convolutional kernels are pre-trained in a layer-wise manner with unsupervised learning followed by fine-tuning the synaptic weights with spike-based supervised gradient descent backpropagation. Our experiments on digit recognition demonstrate that the STDP-based pre-training with gradient-based optimization provides improved robustness, faster (~2.5 {\texttimes}) training time and better generalization compared with purely gradient-based training without pre-training.}
}
@article{conv3,
	title        = {Training Spiking ConvNets by STDP and Gradient Descent},
	author       = {Tavanaei, Amirhossein and Kirby, Zachary and Maida, Anthony S.},
	year         = 2018,
	journal      = {2018 International Joint Conference on Neural Networks (IJCNN)},
	pages        = {1--8}
}
@article{MNIST,
	title        = {Gradient-based learning applied to document recognition},
	author       = {{Lecun}, Y. and {Bottou}, L. and {Bengio}, Y. and {Haffner}, P.},
	year         = 1998,
	journal      = {Proceedings of the IEEE},
	volume       = 86,
	number       = 11,
	pages        = {2278--2324}
}
@article{Lee_2020,
	title        = {Enabling Spike-Based Backpropagation for Training Deep Neural Network Architectures},
	author       = {Lee, Chankyu and Sarwar, Syed Shakib and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
	year         = 2020,
	month        = feb,
	journal      = {Frontiers in Neuroscience},
	publisher    = {Frontiers Media SA},
	volume       = 14,
	doi          = {10.3389/fnins.2020.00119},
	issn         = {1662-453X},
	url          = {http://dx.doi.org/10.3389/fnins.2020.00119}
}
@article{backprop_spiking,
	title        = {Training Deep Spiking Neural Networks Using Backpropagation},
	author       = {Lee, Jun and Delbruck, Tobi and Pfeiffer, Michael},
	year         = 2016,
	month        = {08},
	journal      = {Frontiers in Neuroscience},
	volume       = 10,
	pages        = {},
	doi          = {10.3389/fnins.2016.00508}
}
@article{pehlevan2019spiking,
	title        = {A Spiking Neural Network with Local Learning Rules Derived From Nonnegative Similarity Matching},
	author       = {Cengiz Pehlevan},
	year         = 2019,
	eprint       = {1902.01429},
	archiveprefix = {arXiv},
	primaryclass = {cs.NE}
}
@article{Baldi_2016,
	title        = {A theory of local learning, the learning channel, and the optimality of backpropagation},
	author       = {Baldi, Pierre and Sadowski, Peter},
	year         = 2016,
	month        = nov,
	journal      = {Neural Networks},
	publisher    = {Elsevier BV},
	volume       = 83,
	pages        = {51–74},
	doi          = {10.1016/j.neunet.2016.07.006},
	issn         = {0893-6080},
	url          = {http://dx.doi.org/10.1016/j.neunet.2016.07.006}
}
@article{Khan_2020,
	title        = {A survey of the recent architectures of deep convolutional neural networks},
	author       = {Khan, Asifullah and Sohail, Anabia and Zahoora, Umme and Qureshi, Aqsa Saeed},
	year         = 2020,
	month        = apr,
	journal      = {Artificial Intelligence Review},
	publisher    = {Springer Science and Business Media LLC},
	doi          = {10.1007/s10462-020-09825-6},
	issn         = {1573-7462},
	url          = {http://dx.doi.org/10.1007/s10462-020-09825-6}
}
@article{Edwards2015GrowingPF,
	title        = {Growing pains for deep learning},
	author       = {Chris Edwards},
	year         = 2015,
	journal      = {Commun. ACM},
	volume       = 58,
	pages        = {14--16}
}
@article{human_brain,
	title        = {Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain},
	author       = {Azevedo, Frederico A.C. and Carvalho, Ludmila R.B. and Grinberg, Lea T. and Farfel, José Marcelo and Ferretti, Renata E.L. and Leite, Renata E.P. and Filho, Wilson Jacob and Lent, Roberto and Herculano-Houzel, Suzana},
	year         = 2009,
	journal      = {Journal of Comparative Neurology},
	volume       = 513,
	number       = 5,
	pages        = {532--541},
	doi          = {10.1002/cne.21974},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cne.21974},
	keywords     = {human, brain size, neuron numbers, glia/neuron ratio, evolution, comparative neuroanatomy},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.21974},
	abstract     = {Abstract The human brain is often considered to be the most cognitively capable among mammalian brains and to be much larger than expected for a mammal of our body size. Although the number of neurons is generally assumed to be a determinant of computational power, and despite the widespread quotes that the human brain contains 100 billion neurons and ten times more glial cells, the absolute number of neurons and glial cells in the human brain remains unknown. Here we determine these numbers by using the isotropic fractionator and compare them with the expected values for a human-sized primate. We find that the adult male human brain contains on average 86.1 ± 8.1 billion NeuN-positive cells (“neurons”) and 84.6 ± 9.8 billion NeuN-negative (“nonneuronal”) cells. With only 19\% of all neurons located in the cerebral cortex, greater cortical size (representing 82\% of total brain mass) in humans compared with other primates does not reflect an increased relative number of cortical neurons. The ratios between glial cells and neurons in the human brain structures are similar to those found in other primates, and their numbers of cells match those expected for a primate of human proportions. These findings challenge the common view that humans stand out from other primates in their brain composition and indicate that, with regard to numbers of neuronal and nonneuronal cells, the human brain is an isometrically scaled-up primate brain. J. Comp. Neurol. 513:532–541, 2009. © 2009 Wiley-Liss, Inc.}
}
@article{Ismail_Fawaz_2019,
	title        = {Deep learning for time series classification: a review},
	author       = {Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
	year         = 2019,
	month        = mar,
	journal      = {Data Mining and Knowledge Discovery},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 33,
	number       = 4,
	pages        = {917–963},
	doi          = {10.1007/s10618-019-00619-1},
	issn         = {1573-756X},
	url          = {http://dx.doi.org/10.1007/s10618-019-00619-1}
}
@misc{li2020survey,
	title        = {A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects},
	author       = {Zewen Li and Wenjie Yang and Shouheng Peng and Fan Liu},
	year         = 2020,
	eprint       = {2004.02806},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@article{Li_2018,
	title        = {Review of memristor devices in neuromorphic computing: materials sciences and device challenges},
	author       = {Yibo Li and Zhongrui Wang and Rivu Midya and Qiangfei Xia and J Joshua Yang},
	year         = 2018,
	month        = sep,
	journal      = {Journal of Physics D: Applied Physics},
	publisher    = {{IOP} Publishing},
	volume       = 51,
	number       = 50,
	pages        = 503002,
	doi          = {10.1088/1361-6463/aade3f},
	url          = {https://doi.org/10.1088%2F1361-6463%2Faade3f},
	abstract     = {The memristor is considered as the one of the promising candidates for next generation computing systems. Novel computing architectures based on memristors have shown great potential in replacing or complementing conventional computing platforms based on the von Neumann architecture which faces challenges in the big-data era such as the memory wall. However, there are a number of technical challenges in implementing memristor based computing. In this review, we focus on the research performed on the memristor material stacks and their compatibility with CMOS processes, the electrical performance, and the integration. In addition, recent demonstrations of neuromorphic computing using memristors are surveyed.}
}
@article{brown2020language,
	title        = {Language Models are Few-Shot Learners},
	author       = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
	year         = 2020,
	eprint       = {2005.14165},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{anti-STDP,
	title        = {Anti-Hebbian Spike-Timing-Dependent Plasticity and Adaptive Sensory Processing},
	author       = {Roberts, Patrick and Leen, Todd},
	year         = 2010,
	month        = 12,
	journal      = {Frontiers in computational neuroscience},
	volume       = 4,
	pages        = 156,
	doi          = {10.3389/fncom.2010.00156}
}
@article{10.1007/978-3-030-30425-6_30,
	title        = {Competitive Maximization of Neuronal Activity in Convolutional Recurrent Spiking Neural Networks},
	author       = {Nekhaev, Dmitry and Demin, Vyacheslav},
	year         = 2020,
	booktitle    = {Advances in Neural Computation, Machine Learning, and Cognitive Research III},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {255--262},
	isbn         = {978-3-030-30425-6},
	editor       = {Kryzhanovsky, Boris and Dunin-Barkowski, Witali and Redko, Vladimir and Tiumentsev, Yury},
	abstract     = {Spiking neural networks (SNNs) are the promising algorithm for specific neurochip hardware real-time solutions. SNNs are believed to be highly energy and computationally efficient. We focus on developing local learning rules that are capable to provide both supervised and unsupervised learning. We suppose that each neuron in a biological neural network tends to maximize its activity in competition with other neurons. This principle was put at the basis of SNN learning algorithm called FEELING. Here we introduce efficient Convolutional Recurrent Spiking Neural Network architecture that uses FEELING rules and provides better results than fully connected SNN on MNIST benchmark having 55 times less learnable weight parameters.}
}
@article{contrastive_loss,
	title        = {Dimensionality Reduction by Learning an Invariant Mapping},
	author       = {R. {Hadsell} and S. {Chopra} and Y. {LeCun}},
	year         = 2006,
	booktitle    = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
	volume       = 2,
	number       = {},
	pages        = {1735--1742},
	doi          = {10.1109/CVPR.2006.100}
}
@article{WANG2020101809,
	title        = {Integration and Co-design of Memristive Devices and Algorithms for Artificial Intelligence},
	author       = {Wei Wang and Wenhao Song and Peng Yao and Yang Li and Joseph {Van Nostrand} and Qinru Qiu and Daniele Ielmini and J. Joshua Yang},
	year         = 2020,
	journal      = {iScience},
	volume       = 23,
	number       = 12,
	pages        = 101809,
	doi          = {https://doi.org/10.1016/j.isci.2020.101809},
	issn         = {2589-0042},
	url          = {https://www.sciencedirect.com/science/article/pii/S2589004220310063},
	keywords     = {Computer Architecture, Hardware Co-design, Materials Science},
	abstract     = {Summary Memristive devices share remarkable similarities to biological synapses, dendrites, and neurons at both the physical mechanism level and unit functionality level, making the memristive approach to neuromorphic computing a promising technology for future artificial intelligence. However, these similarities do not directly transfer to the success of efficient computation without device and algorithm co-designs and optimizations. Contemporary deep learning algorithms demand the memristive artificial synapses to ideally possess analog weighting and linear weight-update behavior, requiring substantial device-level and circuit-level optimization. Such co-design and optimization have been the main focus of memristive neuromorphic engineering, which often abandons the “non-ideal” behaviors of memristive devices, although many of them resemble what have been observed in biological components. Novel brain-inspired algorithms are being proposed to utilize such behaviors as unique features to further enhance the efficiency and intelligence of neuromorphic computing, which calls for collaborations among electrical engineers, computing scientists, and neuroscientists.}
}
@article{DEMIN202164,
	title        = {Necessary conditions for STDP-based pattern recognition learning in a memristive spiking neural network},
	author       = {V.A. Demin and D.V. Nekhaev and I.A. Surazhevsky and K.E. Nikiruy and A.V. Emelyanov and S.N. Nikolaev and V.V. Rylkov and M.V. Kovalchuk},
	year         = 2021,
	journal      = {Neural Networks},
	volume       = 134,
	pages        = {64--75},
	doi          = {https://doi.org/10.1016/j.neunet.2020.11.005},
	issn         = {0893-6080},
	url          = {https://www.sciencedirect.com/science/article/pii/S0893608020303907},
	keywords     = {Memristor, Hardware analog neuron, Spiking neural network, Memristive STDP, Unsupervised learning, Probabilistic generative model},
	abstract     = {This work is aimed to study experimental and theoretical approaches for searching effective local training rules for unsupervised pattern recognition by high-performance memristor-based Spiking Neural Networks (SNNs). First, the possibility of weight change using Spike-Timing-Dependent Plasticity (STDP) is demonstrated with a pair of hardware analog neurons connected through a (CoFeB)x(LiNbO3)1−x nanocomposite memristor. Next, the learning convergence to a solution of binary clusterization task is analyzed in a wide range of memristive STDP parameters for a single-layer fully connected feedforward SNN. The memristive STDP behavior supplying convergence in this simple task is shown also to provide it in the handwritten digit recognition domain by the more complex SNN architecture with a Winner-Take-All competition between neurons. To investigate basic conditions necessary for training convergence, an original probabilistic generative model of a rate-based single-layer network with independent or competing neurons is built and thoroughly analyzed. The main result is a statement of “correlation growth-anticorrelation decay” principle which prompts near-optimal policy to configure model parameters. This principle is in line with requiring the binary clusterization convergence which can be defined as the necessary condition for optimal learning and used as the simple benchmark for tuning parameters of various neural network realizations with population-rate information coding. At last, a heuristic algorithm is described to experimentally find out the convergence conditions in a memristive SNN, including robustness to a device variability. Due to the generality of the proposed approach, it can be applied to a wide range of memristors and neurons of software- or hardware-based rate-coding single-layer SNNs when searching for local rules that ensure their unsupervised learning convergence in a pattern recognition task domain.}
}
@article{Emelyanov_2019,
	title        = {Self-adaptive {STDP}-based learning of a spiking neuron with nanocomposite memristive weights},
	author       = {A V Emelyanov and K E Nikiruy and A V Serenko and A V Sitnikov and M Yu Presnyakov and R B Rybka and A G Sboev and V V Rylkov and P K Kashkarov and M V Kovalchuk and V A Demin},
	year         = 2019,
	month        = oct,
	journal      = {Nanotechnology},
	publisher    = {{IOP} Publishing},
	volume       = 31,
	number       = 4,
	pages        = {045201},
	doi          = {10.1088/1361-6528/ab4a6d},
	url          = {https://doi.org/10.1088/1361-6528/ab4a6d},
	abstract     = {Neuromorphic systems consisting of artificial neurons and memristive synapses could provide a much better performance and a significantly more energy-efficient approach to the implementation of different types of neural network algorithms than traditional hardware with the Von-Neumann architecture. However, the memristive weight adjustment in the formal neuromorphic networks by the standard back-propagation techniques suffers from poor device-to-device reproducibility. One of the most promising approaches to overcome this problem is to use local learning rules for spiking neuromorphic architectures which potentially could be adaptive to the variability issue mentioned above. Different kinds of local rules for learning spiking systems are mostly realized on a bio-inspired spike-time-dependent plasticity (STDP) mechanism, which is an improved type of classical Hebbian learning. Whereas the STDP-like mechanism has already been shown to emerge naturally in memristive devices, the demonstration of its self-adaptive learning property, potentially overcoming the variability problem, is more challenging and has yet to be reported. Here we experimentally demonstrate an STDP-based learning protocol that ensures self-adaptation of the memristor resistive states, after only a very few spikes, and makes the plasticity sensitive only to the input signal configuration, but neither to the initial state of the devices nor their device-to-device variability. Then, it is shown that the self-adaptive learning of a spiking neuron with memristive weights on rate-coded patterns could also be realized with hardware-based STDP rules. The experiments have been carried out with nanocomposite-based (Co40Fe40B20)х(LiNbO3−y)100−х memristive structures, but their results are believed to be applicable to a wide range of memristive devices. All the experimental data were supported and extended by numerical simulations. There is a hope that the obtained results pave the way for building up reliable spiking neuromorphic systems composed of partially unreliable analog memristive elements, with a more complex architecture and the capability of unsupervised learning.}
}
@article{SURAZHEVSKY2021110890,
	title        = {Noise-assisted persistence and recovery of memory state in a memristive spiking neuromorphic network},
	author       = {I.A. Surazhevsky and V.A. Demin and A.I. Ilyasov and A.V. Emelyanov and K.E. Nikiruy and V.V. Rylkov and S.A. Shchanikov and I.A. Bordanov and S.A. Gerasimova and D.V. Guseinov and N.V. Malekhonova and D.A. Pavlov and A.I. Belov and A.N. Mikhaylov and V.B. Kazantsev and D. Valenti and B. Spagnolo and M.V. Kovalchuk},
	year         = 2021,
	journal      = {Chaos, Solitons \& Fractals},
	volume       = 146,
	pages        = 110890,
	doi          = {https://doi.org/10.1016/j.chaos.2021.110890},
	issn         = {0960-0779},
	url          = {https://www.sciencedirect.com/science/article/pii/S0960077921002435},
	abstract     = {We investigate the constructive role of an external noise signal, in the form of a low-rate Poisson sequence of pulses supplied to all inputs of a spiking neural network, consisting in maintaining for a long time or even recovering a memory trace (engram) of the image without its direct renewal (or rewriting). In particular, this unique dynamic property is demonstrated in a single-layer spiking neural network consisting of simple integrate-and-fire neurons and memristive synaptic weights. This is carried out by preserving and even fine-tuning the conductance values of memristors in terms of dynamic plasticity, specifically spike-timing-dependent plasticity-type, driven by overlapping pre- and postsynaptic voltage spikes. It has been shown that the weights can be to a certain extent unreliable, due to such characteristics as the limited retention time of resistive state or the variation of switching voltages. Such a noise-assisted persistence of memory, on one hand, could be a prototypical mechanism in a biological nervous system and, on the other hand, brings one step closer to the possibility of building reliable spiking neural networks composed of unreliable analog elements.}
}
@article{hippocampus,
	title        = {A neurohybrid memristive system for adaptive stimulation of hippocampus},
	author       = {Gerasimova, S.A. and Lebedeva, Albina and Fedulina, Anastasia and Koryazhkina, Maria and Belov, A and Mishchenko, M and Matveeva, M and Guseinov, Davud and Mikhaylov, Alexey and Kazantsev, Victor and Pisarchik, Alexander},
	year         = 2021,
	month        = {03},
	journal      = {Chaos Solitons \& Fractals},
	volume       = 146,
	pages        = 110804,
	doi          = {10.1016/j.chaos.2021.110804}
}
@article{nonlinearmemristivedynamics,
	title        = {Non-linear Memristive Synaptic Dynamics for Efficient Unsupervised Learning in Spiking Neural Networks},
	author       = {Brivio, Stefano and Ly, Denys and Vianello, E. and Spiga, Sabina},
	year         = 2021,
	month        = {02},
	journal      = {Frontiers in Neuroscience},
	volume       = 15,
	pages        = 580909,
	doi          = {10.3389/fnins.2021.580909}
}
@inbook{stdpbasedclassification,
	title        = {STDP-Based Classificational Spiking Neural Networks Combining Rate and Temporal Coding},
	author       = {Sboev, Alexander and Vlasov, Danila and Serenko, Alexey and Rybka, Roman and Moloshnikov, Ivan},
	year         = 2021,
	month        = {01},
	pages        = {403--411},
	doi          = {10.1007/978-3-030-60577-3_48},
	isbn         = {978-3-030-60576-6}
}
