@article{neuronmodels,
	author = {Abusnaina, Ahmed and Abdullah, Rosni},
	journal = {International Journal of Digital Content Technology and its Applications},
	month = {06},
	pages = {14--21},
	title = {Spiking Neuron Models: A Review},
	volume = {8},
	year = {2014}
}

@article{ManyParams,
	author = {Alom, Md. Zahangir and Taha, Tarek and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst and Hasan, Mahmudul and Essen, Brian and Awwal, Abdul and Asari, Vijayan},
	doi = {10.3390/electronics8030292},
	journal = {Electronics},
	month = {03},
	pages = {292},
	title = {A State-of-the-Art Survey on Deep Learning Theory and Architectures},
	volume = {8},
	year = {2019}
}

@article{hardware_survey,
	author = {Bouvier, Maxence and Valentian, Alexandre and Mesquida, Thomas and Rummens, Francois and Reyboz, Marina and Vianello, Elisa and Beigne, Edith},
	doi = {10.1145/3304103},
	issn = {1550-4840},
	journal = {ACM Journal on Emerging Technologies in Computing Systems},
	month = {Jun},
	number = {2},
	pages = {1--35},
	publisher = {Association for Computing Machinery (ACM)},
	title = {Spiking Neural Networks Hardware Implementations and Challenges},
	url = {http://dx.doi.org/10.1145/3304103},
	volume = {15},
	year = {2019}
}

@article{Loihi,
	author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Joshi, Prasad and Lines, Andrew and Wild, Andreas and Wang, Hong},
	doi = {10.1109/MM.2018.112130359},
	journal = {IEEE Micro},
	month = {01},
	pages = {1--1},
	title = {Loihi: A Neuromorphic Manycore Processor with On-Chip Learning},
	volume = {PP},
	year = {2018}
}

@article{saunders2019locally,
	archiveprefix = {arXiv},
	author = {Saunders, Daniel J. and Patel, Devdhar and Hazan, Hananel and Siegelmann, Hava T. and Kozma, Robert},
	eprint = {1904.06269},
	primaryclass = {cs.NE},
	title = {Locally Connected Spiking Neural Networks for Unsupervised Feature Learning},
	year = {2019}
}

@article{pmlr-v28-wan13,
	abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.},
	address = {Atlanta, Georgia, USA},
	author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann Le and Fergus, Rob},
	booktitle = {Proceedings of the 30th International Conference on Machine Learning},
	editor = {Dasgupta, Sanjoy and McAllester, David},
	month = {06},
	number = {3},
	pages = {1058--1066},
	pdf = {http://proceedings.mlr.press/v28/wan13.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Regularization of Neural Networks using DropConnect},
	url = {http://proceedings.mlr.press/v28/wan13.html},
	volume = {28},
	year = {2013}
}

@article{MaxActiv1,
	author = {Demin, Vyacheslav and Nekhaev, Dmitry},
	doi = {10.3389/fninf.2018.00079},
	journal = {Frontiers in Neuroinformatics},
	month = {11},
	pages = {79},
	title = {Recurrent Spiking Neural Network Learning Based on a Competitive Maximization of Neuronal Activity},
	volume = {12},
	year = {2018}
}

@article{MaxActiv2,
	author = {Nekhaev, Dmitry and Demin, Vyacheslav},
	doi = {10.1007/978-3-030-30425-6_30},
	isbn = {978-3-030-30424-9},
	month = {01},
	pages = {255--262},
	title = {Competitive Maximization of Neuronal Activity in Convolutional Recurrent Spiking Neural Networks},
	year = {2020}
}

@article{STDP,
	abstract = {How learning and memory is achieved in the brain is a central question in neuroscience. Key to today{\rq}s research into information storage in the brain is the concept of synaptic plasticity, a notion that has been heavily influenced by Hebb's (<xref ref-type="bibr" rid="B97">1949</xref>) postulate. Hebb conjectured that repeatedly and persistently co-active cells should increase connective strength among populations of interconnected neurons as a means of storing a memory trace, also known as an engram. Hebb certainly was not the first to make such a conjecture, as we show in this history. Nevertheless, literally thousands of studies into the classical frequency-dependent paradigm of cellular learning rules were directly inspired by the Hebbian postulate. But in more recent years, a novel concept in cellular learning has emerged, where temporal order instead of frequency is emphasized. This new learning paradigm -- known as spike-timing-dependent plasticity (STDP) -- has rapidly gained tremendous interest, perhaps because of its combination of elegant simplicity, biological plausibility, and computational power. But what are the roots of today{\rq}s STDP concept? Here, we discuss several centuries of diverse thinking, beginning with philosophers such as Aristotle, Locke, and Ribot, traversing, e.g., Lugaro{\rq}s plasticit{\`a} and Rosenblatt{\rq}s perceptron, and culminating with the discovery of STDP. We highlight interactions between theoretical and experimental fields, showing how discoveries sometimes occurred in parallel, seemingly without much knowledge of the other field, and sometimes via concrete back-and-forth communication. We point out where the future directions may lie, which includes interneuron STDP, the functional impact of STDP, its mechanisms and its neuromodulatory regulation, and the linking of STDP to the developmental formation and continuous plasticity of neuronal networks.},
	author = {Markram, Henry and Gerstner, Wulfram and Sj{\"o}str{\"o}m, Per Jesper},
	doi = {10.3389/fnsyn.2011.00004},
	issn = {1663-3563},
	journal = {Frontiers in Synaptic Neuroscience},
	pages = {4},
	title = {A History of Spike-Timing-Dependent Plasticity},
	url = {https://www.frontiersin.org/article/10.3389/fnsyn.2011.00004},
	volume = {3},
	year = {2011}
}

@article{LOBO202088,
	abstract = {Applications that generate huge amounts of data in the form of fast streams are becoming increasingly prevalent, being therefore necessary to learn in an online manner. These conditions usually impose memory and processing time restrictions, and they often turn into evolving environments where a change may affect the input data distribution. Such a change causes that predictive models trained over these stream data become obsolete and do not adapt suitably to new distributions. Specially in these non-stationary scenarios, there is a pressing need for new algorithms that adapt to these changes as fast as possible, while maintaining good performance scores. Unfortunately, most off-the-shelf classification models need to be retrained if they are used in changing environments, and fail to scale properly. Spiking Neural Networks have revealed themselves as one of the most successful approaches to model the behavior and learning potential of the brain, and exploit them to undertake practical online learning tasks. Besides, some specific flavors of Spiking Neural Networks can overcome the necessity of retraining after a drift occurs. This work intends to merge both fields by serving as a comprehensive overview, motivating further developments that embrace Spiking Neural Networks for online learning scenarios, and being a friendly entry point for non-experts.},
	author = {Lobo, Jesus L. and Ser], Javier [Del and Bifet, Albert and Kasabov, Nikola},
	doi = {10.1016/j.neunet.2019.09.004},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Online learning, Spiking Neural Networks, Stream data, Concept drift},
	pages = {88--100},
	title = {Spiking Neural Networks and online learning: An overview and perspectives},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608019302655},
	volume = {121},
	year = {2020}
}

@article{hardware1,
	abstract = {Spiking Neural Network (SNN) is a type of biologically-inspired neural networks that perform information processing based on discrete-time spikes, different from traditional Artificial Neural Network (ANN). Hardware implementation of SNNs is necessary for achieving high-performance and low-power. We present the Darwin Neural Processing Unit (NPU), a highly-configurable neuromorphic hardware co-processor based on SNN implemented with digital logic, supporting a configurable number of neurons, synapses and synaptic delays. The Darwin NPU was fabricated by standard 180nm CMOS technology with area size of 5\,{\texttimes}\,5 mm2 and 70MHz clock frequency at the worst case. It consumes 0.84mW/MHz with 1.8V power supply for typical applications. Two prototype applications are used to demonstrate the performance and efficiency of the Darwin NPU.},
	author = {Ma, De and Shen, Juncheng and Gu, Zonghua and Zhang, Ming and Zhu, Xiaolei and Xu, Xiaoqiang and Xu, Qi and Shen, Yangjing and Pan, Gang},
	doi = {10.1016/j.sysarc.2017.01.003},
	issn = {1383-7621},
	journal = {Journal of Systems Architecture},
	keywords = {Neuromorphic computing, Spiking neural networks (SNN), Address-event representation (AER), Digital VLSI},
	pages = {43--51},
	title = {Darwin: A neuromorphic hardware co-processor based on spiking neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S1383762117300231},
	volume = {77},
	year = {2017}
}

@article{hardware2,
	archiveprefix = {arXiv},
	author = {Tang, Guangzhi and Shah, Arpit and Michmizos, Konstantinos P.},
	eprint = {1903.02504},
	primaryclass = {cs.RO},
	title = {Spiking Neural Network on Neuromorphic Hardware for Energy-Efficient Unidimensional SLAM},
	year = {2019}
}

@article{mnist1,
	author = {Kulkarni, Shruti and Rajendran, Bipin},
	doi = {10.1016/j.neunet.2018.03.019},
	journal = {Neural Networks},
	month = {04},
	pages = {},
	title = {Spiking neural networks for handwritten digit recognition---Supervised learning and network optimization},
	volume = {103},
	year = {2018}
}

@article{TrueNorth,
	abstract = {Computers are nowhere near as versatile as our own brains. Merolla et al. applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.Science, this issue p. 668 Inspired by the brain{\rq}s structure, we have developed an efficient, scalable, and flexible non--von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
	author = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
	doi = {10.1126/science.1254642},
	eprint = {https://science.sciencemag.org/content/345/6197/668.full.pdf},
	issn = {0036-8075},
	journal = {Science},
	number = {6197},
	pages = {668--673},
	publisher = {American Association for the Advancement of Science},
	title = {A million spiking-neuron integrated circuit with a scalable communication network and interface},
	url = {https://science.sciencemag.org/content/345/6197/668},
	volume = {345},
	year = {2014}
}

@article{SpiNNaker,
	author = {Painkras, Eustace and Plana, Luis and Garside, Jim and Temple, Steve and Galluppi, Francesco and Patterson, Cameron and Lester, David and Brown, Andrew and Furber, Steve},
	doi = {10.1109/JSSC.2013.2259038},
	journal = {Solid-State Circuits, IEEE Journal of},
	month = {08},
	pages = {1943--1953},
	title = {SpiNNaker: A 1-W 18-Core System-on-Chip for Massively-Parallel Neural Network Simulation},
	volume = {48},
	year = {2013}
}

@article{Akida,
	month = {Apr},
	title = {Akida Neural Processor System-on-Chip},
	url = {https://brainchipinc.com/akida-neuromorphic-system-on-chip/},
	year = {2020}
}

@article{mnist2,
	abstract = {In order to understand how the mammalian neocortex is performing computations, two things are necessary; we need to have a good understanding of the available neuronal processing units and mechanisms, and we need to gain a better understanding of how those mechanisms are combined to build functioning systems. Therefore, in recent years there is an increasing interest in how spiking neural networks (SNN) can be used to perform complex computations or solve pattern recognition tasks. However, it remains a challenging task to design SNNs which use biologically plausible mechanisms (especially for learning new patterns), since most such SNN architectures rely on training in a rate-based network and subsequent conversion to a SNN. We present a SNN for digit recognition which is based on mechanisms with increased biological plausibility, i.e., conductance-based instead of current-based synapses, spike-timing-dependent plasticity with time-dependent weight change, lateral inhibition, and an adaptive spiking threshold. Unlike most other systems, we do not use a teaching signal and do not present any class labels to the network. Using this unsupervised learning scheme, our architecture achieves 95\% accuracy on the MNIST benchmark, which is better than previous SNN implementations without supervision. The fact that we used no domain-specific knowledge points toward the general applicability of our network design. Also, the performance of our network scales well with the number of neurons used and shows similar performance for four different learning rules, indicating robustness of the full combination of mechanisms, which suggests applicability in heterogeneous biological neural networks.},
	author = {Diehl, Peter and Cook, Matthew},
	doi = {10.3389/fncom.2015.00099},
	issn = {1662-5188},
	journal = {Frontiers in Computational Neuroscience},
	pages = {99},
	title = {Unsupervised learning of digit recognition using spike-timing-dependent plasticity},
	url = {https://www.frontiersin.org/article/10.3389/fncom.2015.00099},
	volume = {9},
	year = {2015}
}

@article{conv1,
	author = {{Tavanaei}, A. and {Maida}, A. S.},
	booktitle = {2017 International Joint Conference on Neural Networks (IJCNN)},
	number = {},
	pages = {2023--2030},
	title = {Multi-layer unsupervised learning in a spiking convolutional neural network},
	volume = {},
	year = {2017}
}

@article{conv2,
	abstract = {Spiking Neural Networks (SNNs) are fast becoming a promising candidate for brain-inspired neuromorphic computing because of their inherent power efficiency and impressive inference accuracy across several cognitive tasks such as image classification and speech recognition. The recent efforts in SNNs have been focused on implementing deeper networks with multiple hidden layers to incorporate exponentially more difficult functional representations. In this paper, we propose a pre-training scheme using biologically plausible unsupervised learning, namely Spike-Timing-Dependent-Plasticity (STDP), in order to better initialize the parameters in multi-layer systems prior to supervised optimization. The multi-layer SNN is comprised of alternating convolutional and pooling layers followed by fully-connected layers, which are populated with leaky integrate-and-fire spiking neurons. We train the deep SNNs in two phases wherein, first, convolutional kernels are pre-trained in a layer-wise manner with unsupervised learning followed by fine-tuning the synaptic weights with spike-based supervised gradient descent backpropagation. Our experiments on digit recognition demonstrate that the STDP-based pre-training with gradient-based optimization provides improved robustness, faster (~2.5 {\texttimes}) training time and better generalization compared with purely gradient-based training without pre-training.},
	author = {Lee, Chankyu and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
	doi = {10.3389/fnins.2018.00435},
	issn = {1662-453X},
	journal = {Frontiers in Neuroscience},
	pages = {435},
	title = {Training Deep Spiking Convolutional Neural Networks With STDP-Based Unsupervised Pre-training Followed by Supervised Fine-Tuning},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2018.00435},
	volume = {12},
	year = {2018}
}

@article{conv3,
	author = {Tavanaei, Amirhossein and Kirby, Zachary and Maida, Anthony S.},
	journal = {2018 International Joint Conference on Neural Networks (IJCNN)},
	pages = {1--8},
	title = {Training Spiking ConvNets by STDP and Gradient Descent},
	year = {2018}
}

@article{MNIST,
	author = {{Lecun}, Y. and {Bottou}, L. and {Bengio}, Y. and {Haffner}, P.},
	journal = {Proceedings of the IEEE},
	number = {11},
	pages = {2278--2324},
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	year = {1998}
}

@article{Lee_2020,
   title={Enabling Spike-Based Backpropagation for Training Deep Neural Network Architectures},
   volume={14},
   ISSN={1662-453X},
   url={http://dx.doi.org/10.3389/fnins.2020.00119},
   DOI={10.3389/fnins.2020.00119},
   journal={Frontiers in Neuroscience},
   publisher={Frontiers Media SA},
   author={Lee, Chankyu and Sarwar, Syed Shakib and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
   year={2020},
   month={Feb}
}

@article{backprop_spiking,
author = {Lee, Jun and Delbruck, Tobi and Pfeiffer, Michael},
year = {2016},
month = {08},
pages = {},
title = {Training Deep Spiking Neural Networks Using Backpropagation},
volume = {10},
journal = {Frontiers in Neuroscience},
doi = {10.3389/fnins.2016.00508}
}

@article{pehlevan2019spiking,
    title={A Spiking Neural Network with Local Learning Rules Derived From Nonnegative Similarity Matching},
    author={Cengiz Pehlevan},
    year={2019},
    eprint={1902.01429},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}

@article{Baldi_2016,
   title={A theory of local learning, the learning channel, and the optimality of backpropagation},
   volume={83},
   ISSN={0893-6080},
   url={http://dx.doi.org/10.1016/j.neunet.2016.07.006},
   DOI={10.1016/j.neunet.2016.07.006},
   journal={Neural Networks},
   publisher={Elsevier BV},
   author={Baldi, Pierre and Sadowski, Peter},
   year={2016},
   month={Nov},
   pages={51â€“74}
}

@article{Khan_2020,
   title={A survey of the recent architectures of deep convolutional neural networks},
   ISSN={1573-7462},
   url={http://dx.doi.org/10.1007/s10462-020-09825-6},
   DOI={10.1007/s10462-020-09825-6},
   journal={Artificial Intelligence Review},
   publisher={Springer Science and Business Media LLC},
   author={Khan, Asifullah and Sohail, Anabia and Zahoora, Umme and Qureshi, Aqsa Saeed},
   year={2020},
   month={Apr}
}

@article{Edwards2015GrowingPF,
  title={Growing pains for deep learning},
  author={Chris Edwards},
  journal={Commun. ACM},
  year={2015},
  volume={58},
  pages={14-16}
}
