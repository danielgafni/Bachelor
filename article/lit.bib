@misc{saunders2019locally,
	archiveprefix = {arXiv},
	author = {Saunders, Daniel J. and Patel, Devdhar and Hazan, Hananel and Siegelmann, Hava T. and Kozma, Robert},
	eprint = {1904.06269},
	primaryclass = {cs.NE},
	title = {Locally Connected Spiking Neural Networks for Unsupervised Feature Learning},
	year = {2019}
}

@inproceedings{pmlr-v28-wan13,
	abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.},
	address = {Atlanta, Georgia, USA},
	author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann Le and Fergus, Rob},
	booktitle = {Proceedings of the 30th International Conference on Machine Learning},
	editor = {Dasgupta, Sanjoy and McAllester, David},
	month = {17--19 Jun},
	number = {3},
	pages = {1058--1066},
	pdf = {http://proceedings.mlr.press/v28/wan13.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Regularization of Neural Networks using DropConnect},
	url = {http://proceedings.mlr.press/v28/wan13.html},
	volume = {28},
	year = {2013}
}

@article{MaxActiv1,
	author = {Demin, Vyacheslav and Nekhaev, Dmitry},
	doi = {10.3389/fninf.2018.00079},
	journal = {Frontiers in Neuroinformatics},
	month = {11},
	pages = {79},
	title = {Recurrent Spiking Neural Network Learning Based on a Competitive Maximization of Neuronal Activity},
	volume = {12},
	year = {2018}
}

@inbook{MaxActiv2,
	author = {Nekhaev, Dmitry and Demin, Vyacheslav},
	doi = {10.1007/978-3-030-30425-6_30},
	isbn = {978-3-030-30424-9},
	month = {01},
	pages = {255--262},
	title = {Competitive Maximization of Neuronal Activity in Convolutional Recurrent Spiking Neural Networks},
	year = {2020}
}

@article{ManyParams,
author = {Alom, Md. Zahangir and Taha, Tarek and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst and Hasan, Mahmudul and Essen, Brian and Awwal, Abdul and Asari, Vijayan},
year = {2019},
month = {03},
pages = {292},
title = {A State-of-the-Art Survey on Deep Learning Theory and Architectures},
volume = {8},
journal = {Electronics},
doi = {10.3390/electronics8030292}
}

@ARTICLE{STDP,
  
AUTHOR={Markram, Henry and Gerstner, Wulfram and Sjöström, Per Jesper},   
	 
TITLE={A History of Spike-Timing-Dependent Plasticity},      
	
JOURNAL={Frontiers in Synaptic Neuroscience},      
	
VOLUME={3},      

PAGES={4},     
	
YEAR={2011},      
	  
URL={https://www.frontiersin.org/article/10.3389/fnsyn.2011.00004},       
	
DOI={10.3389/fnsyn.2011.00004},      
	
ISSN={1663-3563},   
   
ABSTRACT={How learning and memory is achieved in the brain is a central question in neuroscience. Key to today’s research into information storage in the brain is the concept of synaptic plasticity, a notion that has been heavily influenced by Hebb's (<xref ref-type="bibr" rid="B97">1949</xref>) postulate. Hebb conjectured that repeatedly and persistently co-active cells should increase connective strength among populations of interconnected neurons as a means of storing a memory trace, also known as an engram. Hebb certainly was not the first to make such a conjecture, as we show in this history. Nevertheless, literally thousands of studies into the classical frequency-dependent paradigm of cellular learning rules were directly inspired by the Hebbian postulate. But in more recent years, a novel concept in cellular learning has emerged, where temporal order instead of frequency is emphasized. This new learning paradigm – known as spike-timing-dependent plasticity (STDP) – has rapidly gained tremendous interest, perhaps because of its combination of elegant simplicity, biological plausibility, and computational power. But what are the roots of today’s STDP concept? Here, we discuss several centuries of diverse thinking, beginning with philosophers such as Aristotle, Locke, and Ribot, traversing, e.g., Lugaro’s plasticità and Rosenblatt’s perceptron, and culminating with the discovery of STDP. We highlight interactions between theoretical and experimental fields, showing how discoveries sometimes occurred in parallel, seemingly without much knowledge of the other field, and sometimes via concrete back-and-forth communication. We point out where the future directions may lie, which includes interneuron STDP, the functional impact of STDP, its mechanisms and its neuromodulatory regulation, and the linking of STDP to the developmental formation and continuous plasticity of neuronal networks.}
}

@article{LOBO202088,
title = {Spiking Neural Networks and online learning: An overview and perspectives},
journal = {Neural Networks},
volume = {121},
pages = {88 - 100},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.004},
url = {http://www.sciencedirect.com/science/article/pii/S0893608019302655},
author = {Jesus L. Lobo and Javier [Del Ser] and Albert Bifet and Nikola Kasabov},
keywords = {Online learning, Spiking Neural Networks, Stream data, Concept drift},
abstract = {Applications that generate huge amounts of data in the form of fast streams are becoming increasingly prevalent, being therefore necessary to learn in an online manner. These conditions usually impose memory and processing time restrictions, and they often turn into evolving environments where a change may affect the input data distribution. Such a change causes that predictive models trained over these stream data become obsolete and do not adapt suitably to new distributions. Specially in these non-stationary scenarios, there is a pressing need for new algorithms that adapt to these changes as fast as possible, while maintaining good performance scores. Unfortunately, most off-the-shelf classification models need to be retrained if they are used in changing environments, and fail to scale properly. Spiking Neural Networks have revealed themselves as one of the most successful approaches to model the behavior and learning potential of the brain, and exploit them to undertake practical online learning tasks. Besides, some specific flavors of Spiking Neural Networks can overcome the necessity of retraining after a drift occurs. This work intends to merge both fields by serving as a comprehensive overview, motivating further developments that embrace Spiking Neural Networks for online learning scenarios, and being a friendly entry point for non-experts.}
}

@article{hardware1,
title = "Darwin: A neuromorphic hardware co-processor based on spiking neural networks",
journal = "Journal of Systems Architecture",
volume = "77",
pages = "43 - 51",
year = "2017",
issn = "1383-7621",
doi = "https://doi.org/10.1016/j.sysarc.2017.01.003",
url = "http://www.sciencedirect.com/science/article/pii/S1383762117300231",
author = "De Ma and Juncheng Shen and Zonghua Gu and Ming Zhang and Xiaolei Zhu and Xiaoqiang Xu and Qi Xu and Yangjing Shen and Gang Pan",
keywords = "Neuromorphic computing, Spiking neural networks (SNN), Address-event representation (AER), Digital VLSI",
abstract = "Spiking Neural Network (SNN) is a type of biologically-inspired neural networks that perform information processing based on discrete-time spikes, different from traditional Artificial Neural Network (ANN). Hardware implementation of SNNs is necessary for achieving high-performance and low-power. We present the Darwin Neural Processing Unit (NPU), a highly-configurable neuromorphic hardware co-processor based on SNN implemented with digital logic, supporting a configurable number of neurons, synapses and synaptic delays. The Darwin NPU was fabricated by standard 180nm CMOS technology with area size of 5 × 5 mm2 and 70MHz clock frequency at the worst case. It consumes 0.84mW/MHz with 1.8V power supply for typical applications. Two prototype applications are used to demonstrate the performance and efficiency of the Darwin NPU."
}

@misc{hardware2,
    title={Spiking Neural Network on Neuromorphic Hardware for Energy-Efficient Unidimensional SLAM},
    author={Guangzhi Tang and Arpit Shah and Konstantinos P. Michmizos},
    year={2019},
    eprint={1903.02504},
    archivePrefix={arXiv},
    primaryClass={cs.RO}
}

@article{mnist1,
author = {Kulkarni, Shruti and Rajendran, Bipin},
year = {2018},
month = {04},
pages = {},
title = {Spiking neural networks for handwritten digit recognition—Supervised learning and network optimization},
volume = {103},
journal = {Neural Networks},
doi = {10.1016/j.neunet.2018.03.019}
}

@article{neuronmodels,
author = {Abusnaina, Ahmed and Abdullah, Rosni},
year = {2014},
month = {06},
pages = {14-21},
title = {Spiking Neuron Models: A Review},
volume = {8},
journal = {International Journal of Digital Content Technology and its Applications}
}

@article{hardware_survey,
   title={Spiking Neural Networks Hardware Implementations and Challenges},
   volume={15},
   ISSN={1550-4840},
   url={http://dx.doi.org/10.1145/3304103},
   DOI={10.1145/3304103},
   number={2},
   journal={ACM Journal on Emerging Technologies in Computing Systems},
   publisher={Association for Computing Machinery (ACM)},
   author={Bouvier, Maxence and Valentian, Alexandre and Mesquida, Thomas and Rummens, Francois and Reyboz, Marina and Vianello, Elisa and Beigne, Edith},
   year={2019},
   month={Jun},
   pages={1–35}
}

@article {TrueNorth,
	author = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
	title = {A million spiking-neuron integrated circuit with a scalable communication network and interface},
	volume = {345},
	number = {6197},
	pages = {668--673},
	year = {2014},
	doi = {10.1126/science.1254642},
	publisher = {American Association for the Advancement of Science},
	abstract = {Computers are nowhere near as versatile as our own brains. Merolla et al. applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.Science, this issue p. 668 Inspired by the brain{\textquoteright}s structure, we have developed an efficient, scalable, and flexible non{\textendash}von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/345/6197/668},
	eprint = {https://science.sciencemag.org/content/345/6197/668.full.pdf},
	journal = {Science}
}

@article{SpiNNaker,
author = {Painkras, Eustace and Plana, Luis and Garside, Jim and Temple, Steve and Galluppi, Francesco and Patterson, Cameron and Lester, David and Brown, Andrew and Furber, Steve},
year = {2013},
month = {08},
pages = {1943-1953},
title = {SpiNNaker: A 1-W 18-Core System-on-Chip for Massively-Parallel Neural Network Simulation},
volume = {48},
journal = {Solid-State Circuits, IEEE Journal of},
doi = {10.1109/JSSC.2013.2259038}
}

@article{Loihi,
author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Joshi, Prasad and Lines, Andrew and Wild, Andreas and Wang, Hong},
year = {2018},
month = {01},
pages = {1-1},
title = {Loihi: A Neuromorphic Manycore Processor with On-Chip Learning},
volume = {PP},
journal = {IEEE Micro},
doi = {10.1109/MM.2018.112130359}
}

@misc{Akida,
title={Akida Neural Processor System-on-Chip},
url={https://brainchipinc.com/akida-neuromorphic-system-on-chip/},
year={2020},
month={Apr}}

@ARTICLE{mnist1,
  
AUTHOR={Diehl, Peter and Cook, Matthew},   
	 
TITLE={Unsupervised learning of digit recognition using spike-timing-dependent plasticity},      
	
JOURNAL={Frontiers in Computational Neuroscience},      
	
VOLUME={9},      

PAGES={99},     
	
YEAR={2015},      
	  
URL={https://www.frontiersin.org/article/10.3389/fncom.2015.00099},       
	
DOI={10.3389/fncom.2015.00099},      
	
ISSN={1662-5188},   
   
ABSTRACT={In order to understand how the mammalian neocortex is performing computations, two things are necessary; we need to have a good understanding of the available neuronal processing units and mechanisms, and we need to gain a better understanding of how those mechanisms are combined to build functioning systems. Therefore, in recent years there is an increasing interest in how spiking neural networks (SNN) can be used to perform complex computations or solve pattern recognition tasks. However, it remains a challenging task to design SNNs which use biologically plausible mechanisms (especially for learning new patterns), since most such SNN architectures rely on training in a rate-based network and subsequent conversion to a SNN. We present a SNN for digit recognition which is based on mechanisms with increased biological plausibility, i.e., conductance-based instead of current-based synapses, spike-timing-dependent plasticity with time-dependent weight change, lateral inhibition, and an adaptive spiking threshold. Unlike most other systems, we do not use a teaching signal and do not present any class labels to the network. Using this unsupervised learning scheme, our architecture achieves 95% accuracy on the MNIST benchmark, which is better than previous SNN implementations without supervision. The fact that we used no domain-specific knowledge points toward the general applicability of our network design. Also, the performance of our network scales well with the number of neurons used and shows similar performance for four different learning rules, indicating robustness of the full combination of mechanisms, which suggests applicability in heterogeneous biological neural networks.}
}

@INPROCEEDINGS{conv1,

  author={A. {Tavanaei} and A. S. {Maida}},

  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)}, 

  title={Multi-layer unsupervised learning in a spiking convolutional neural network}, 

  year={2017},

  volume={},

  number={},

  pages={2023-2030},}

@ARTICLE{conv2,
  
AUTHOR={Lee, Chankyu and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},   
	 
TITLE={Training Deep Spiking Convolutional Neural Networks With STDP-Based Unsupervised Pre-training Followed by Supervised Fine-Tuning},      
	
JOURNAL={Frontiers in Neuroscience},      
	
VOLUME={12},      

PAGES={435},     
	
YEAR={2018},      
	  
URL={https://www.frontiersin.org/article/10.3389/fnins.2018.00435},       
	
DOI={10.3389/fnins.2018.00435},      
	
ISSN={1662-453X},   
   
ABSTRACT={Spiking Neural Networks (SNNs) are fast becoming a promising candidate for brain-inspired neuromorphic computing because of their inherent power efficiency and impressive inference accuracy across several cognitive tasks such as image classification and speech recognition. The recent efforts in SNNs have been focused on implementing deeper networks with multiple hidden layers to incorporate exponentially more difficult functional representations. In this paper, we propose a pre-training scheme using biologically plausible unsupervised learning, namely Spike-Timing-Dependent-Plasticity (STDP), in order to better initialize the parameters in multi-layer systems prior to supervised optimization. The multi-layer SNN is comprised of alternating convolutional and pooling layers followed by fully-connected layers, which are populated with leaky integrate-and-fire spiking neurons. We train the deep SNNs in two phases wherein, first, convolutional kernels are pre-trained in a layer-wise manner with unsupervised learning followed by fine-tuning the synaptic weights with spike-based supervised gradient descent backpropagation. Our experiments on digit recognition demonstrate that the STDP-based pre-training with gradient-based optimization provides improved robustness, faster (~2.5 ×) training time and better generalization compared with purely gradient-based training without pre-training.}
}

@article{conv3,
  title={Training Spiking ConvNets by STDP and Gradient Descent},
  author={Amirhossein Tavanaei and Zachary Kirby and Anthony S. Maida},
  journal={2018 International Joint Conference on Neural Networks (IJCNN)},
  year={2018},
  pages={1-8}
}

@ARTICLE{MNIST,

  author={Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},

  journal={Proceedings of the IEEE}, 

  title={Gradient-based learning applied to document recognition}, 

  year={1998},

  volume={86},

  number={11},

  pages={2278-2324},}
